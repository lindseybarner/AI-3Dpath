{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protected-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import h5py\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import tifffile\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import copy\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import openslide\n",
    "from preprocessing.datamodel import SlideManager\n",
    "from preprocessing.processing import split_negative_slide, split_positive_slide, create_tumor_mask, rgb2gray\n",
    "from preprocessing.util import TileMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4a820",
   "metadata": {},
   "source": [
    "## Functions to shuffle test patches out of training folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_test_case_patches(patches_dir, test_case):  \n",
    "    \n",
    "    # Make folder to move all test patches to. Model will not see this during training\n",
    "    if os.path.exists(patches_dir + '//validation') == False:\n",
    "        os.mkdir(patches_dir + '//validation//')\n",
    "        os.mkdir(patches_dir + '//validation//0//')\n",
    "        os.mkdir(patches_dir + '//validation//1//')\n",
    "    print('created validation folder for ' + str(test_case) + (' patches'))\n",
    "\n",
    "    ## Move test case 0 patches to the new folder\n",
    "    for file in os.listdir(patches_dir + '//' + 'all//0//'):\n",
    "        if file.startswith(test_case):\n",
    "            source = patches_dir + '//all//0//' + file\n",
    "            dest = patches_dir + '//validation//0//' + file\n",
    "            shutil.move(source, dest) #move to designated path\n",
    "\n",
    "    ## Move test case 1 patches to the new folder\n",
    "    for file in os.listdir(patches_dir + '//all//1//'):\n",
    "        if file.startswith(test_case):\n",
    "            source = patches_dir + '//all//1//' + file\n",
    "            dest = patches_dir + '//validation//1//' + file\n",
    "            shutil.move(source, dest) #move to designated path\n",
    "\n",
    "    print('files for test case ' + str(test_case) + ' all moved, ready to train')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imperial-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_patches(patches_dir, test_case):\n",
    "    ## Move files back to patches//all folder\n",
    "    for file in os.listdir(patches_dir + '//validation//0//'):\n",
    "        if file.startswith(test_case):\n",
    "            source = patches_dir + '//validation//0//' + file\n",
    "            dest = patches_dir + '//all//0//' + file\n",
    "            shutil.move(source, dest) #move to designated path\n",
    "\n",
    "    for file in os.listdir(patches_dir + '//validation//1//'):\n",
    "        if file.startswith(test_case):\n",
    "            source = patches_dir + '//validation//1//' + file\n",
    "            dest = patches_dir + '//' + 'all//1//' + file\n",
    "            shutil.move(source, dest) #move to designated path\n",
    "            \n",
    "    ## Make sure test case folder is empty before deleting directory\n",
    "    if len(os.listdir(patches_dir + '//validation//0//')) == 0 and len(os.listdir(patches_dir + '//validation//1//')) == 0:\n",
    "        print('deleting directory validation//')\n",
    "        os.rmdir(patches_dir + '//validation//0')\n",
    "        os.rmdir(patches_dir + '//validation//1')\n",
    "        os.rmdir(patches_dir + '//validation')\n",
    "    else:\n",
    "        print('test patch directory is not empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bff9cf",
   "metadata": {},
   "source": [
    "## Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fallen-mumbai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotated slides: 17\n",
      "Number of normal slides: 26\n"
     ]
    }
   ],
   "source": [
    "## Define parameters\n",
    "# Define directory and slide manager\n",
    "DIR = ''\n",
    "mgr = SlideManager(cam16_dir=DIR)\n",
    "\n",
    "# Get annotated slides\n",
    "slides_met = mgr.met_slides\n",
    "N_met = len(slides_met)\n",
    "print('Number of annotated slides:', N_met)\n",
    "\n",
    "# Get normal slides\n",
    "slides_negative = mgr.negative_slides\n",
    "N_negative = len(slides_negative)\n",
    "print('Number of normal slides:', N_negative)\n",
    "\n",
    "data_dir = 'patches//'\n",
    "\n",
    "model_name = \"resnet\" #\"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "input_size = 512\n",
    "\n",
    "# Number of epchs to train for\n",
    "num_epochs = 20\n",
    "\n",
    "feature_extract = False\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "valuable-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model function handles training and validation of a given model\n",
    "#inputs: PyTorch model, dictionary of dataloaders, loss function, optimizer,\n",
    "#number of epochs to train/validate for\n",
    "#during training, it keeps track of best performing model in terms of validation\n",
    "#accuracy. at the end, it returns the best performing model\n",
    "\n",
    "def train_model(model, fold, dataloaders_train, criterion, optimizer, num_epochs):\n",
    "    since = time.time()\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        ## Initialize variables\n",
    "        all_labels = None\n",
    "        all_outputs = None\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        dataloaders = dataloaders_train['all']\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        ii = 0\n",
    "        for inputs, labels in dataloaders: #get inputs. data is list of [inputs, labels]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                # Get model outputs and calculate loss\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                positive_predictions = np.where(preds.cpu().detach().numpy() == 1)\n",
    "                negative_predictions = np.where(preds.cpu().detach().numpy() == 0)\n",
    "\n",
    "                labels_cpu = labels.cpu().detach().numpy()\n",
    "                preds_cpu = preds.cpu().detach().numpy()\n",
    "                N_preds = len(labels_cpu)\n",
    "                N_1 = sum(labels_cpu)\n",
    "                N_0 = N_preds - N_1\n",
    "                N_correct = len(np.where(labels_cpu == preds_cpu)[0])\n",
    "\n",
    "                if all_outputs is None:\n",
    "                    all_outputs = outputs.cpu().detach().numpy()\n",
    "                    all_labels = labels_cpu\n",
    "                else:\n",
    "                    all_outputs = np.append(all_outputs, outputs.cpu().detach().numpy(), axis = 0)\n",
    "                    all_labels = np.append(all_labels, labels_cpu)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            ii += 1\n",
    "\n",
    "        epoch_loss = running_loss / (ii*batch_size) #len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / (ii*batch_size) #len(dataloaders[phase].dataset)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        \n",
    "        ## Save models across all epochs\n",
    "        torch.save(model, 'model for fold ' + str(fold) + ', epoch ' + str(epoch) + '.pt') \n",
    "        print('train loss history = ' + str(train_loss_history))\n",
    "\n",
    "    plt.figure\n",
    "    plt.plot(train_loss_history,'b')\n",
    "    plt.show()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return model, train_loss_history, all_outputs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "diverse-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When feature extracting, we only want to update parameters of last layer\n",
    "# Therefore, do not need to compute gradients for the rest of model\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "endless-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        ## Add dropout layer\n",
    "        model_ft.fc = nn.Sequential(nn.Dropout(0.5),nn.Linear(num_ftrs, 2))\n",
    "        ## Register dropout layers as forward hooks (applied to each layer)\n",
    "#         model_ft.fc.register_forward_hook(lambda m, inp, out: torch.nn.functional.dropout(out, p=0.5, training=m.training))\n",
    "#         model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 512 #256# 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 256 #224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 256 #224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Print the model we just instantiated\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "occupational-handle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches in metaplasia-negative training set: 179223\n",
      "Number of patches in metaplasia-positive training set: 18667\n"
     ]
    }
   ],
   "source": [
    "#Find number of files in train//0//\n",
    "path = data_dir + '//all//0//'\n",
    "N_0 = 0\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.tif'):\n",
    "        N_0 += 1\n",
    "\n",
    "print('Number of patches in metaplasia-negative training set: ' + str(N_0))\n",
    "\n",
    "#Find number of files in train//1//\n",
    "N_1 = 0\n",
    "path = data_dir + '//all//1//'\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.tif'):\n",
    "        N_1 += 1\n",
    "print('Number of patches in metaplasia-positive training set: ' + str(N_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-housing",
   "metadata": {},
   "source": [
    "## Identify cases and set up transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "representative-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_dir + '//all//'\n",
    "# dataset = datasets.ImageFolder(root = 'patches//')\n",
    "\n",
    "## Get list of case names and class (0 or 1) for all patches\n",
    "cases = []\n",
    "classes = []\n",
    "\n",
    "dataset = datasets.ImageFolder(root = path)\n",
    "for i in range(len(dataset.imgs)):\n",
    "    patch_name = dataset.imgs[i][0].split('\\\\') ## Grab file name of the patch\n",
    "    patch_case = patch_name[1].split('_')[0] ## Grab case name from file name - relies on \"case_\" format\n",
    "    patch_class = dataset.imgs[i][1] ## Grab class of the patch (0 or 1)\n",
    "\n",
    "    cases.append(patch_case)\n",
    "    classes.append(patch_class)\n",
    "    \n",
    "## Count how many patches correspond to each case. Depends on case names being unique in the list (check this manually)\n",
    "counts = Counter(cases)\n",
    "N_cases = len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beginning-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "urban-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'all': transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=(0, 180)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "#         transforms.Resize((256,256))\n",
    "        transforms.ColorJitter(hue = 0.25),\n",
    "        transforms.ColorJitter(brightness = 0.4),\n",
    "        transforms.ColorJitter(contrast = 0.40),\n",
    "        transforms.ColorJitter(saturation = 0.10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-vietnam",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "engaged-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This generates a list of negative and positive cases. \n",
    "## Some cases have both negative and positive slides, which end up in \"postive_cases\" list\n",
    "## Cases in negative_slides do not have ANY positive slides\n",
    "negative_cases = []\n",
    "positive_cases = []\n",
    "for test_case in Counter(cases): # Each iteration represents a K-fold   \n",
    "    ## Quick loop to determine if case is positive or negative\n",
    "    indices = [i for i, x in enumerate(cases) if x == test_case] ## Find indices of dataset that contain test_case of interest\n",
    "    status = 'negative'\n",
    "    for x in indices:\n",
    "        if classes[x] == 1:\n",
    "            status = 'positive'\n",
    "            break\n",
    "    if status == 'positive':# and test_case not in cases_completed:\n",
    "        positive_cases.append(test_case)\n",
    "    if status == 'negative':\n",
    "        negative_cases.append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-claim",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('13971B1-a', '08122E1-a')\n",
      "created validation folder for 13971B1-a patches\n",
      "files for test case 13971B1-a all moved, ready to train\n",
      "created validation folder for 08122E1-a patches\n",
      "files for test case 08122E1-a all moved, ready to train\n",
      "loading model for fold 1, epoch 4.pt\n",
      "N_0 = 128131\n",
      "N_1 = 17593\n",
      "test case = ('13971B1-a', '08122E1-a')\n",
      "dataloaders_train = {'all': <torch.utils.data.dataloader.DataLoader object at 0x0000019B9C850128>}\n",
      "Epoch 5/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642]\n",
      "val loss history = []\n",
      "Epoch 6/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388]\n",
      "val loss history = []\n",
      "Epoch 7/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388, 0.2198793582223948]\n",
      "val loss history = []\n",
      "Epoch 8/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388, 0.2198793582223948, 0.2081559579917604]\n",
      "val loss history = []\n",
      "Epoch 9/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388, 0.2198793582223948, 0.2081559579917604, 0.19688529690292403]\n",
      "val loss history = []\n",
      "Epoch 10/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388, 0.2198793582223948, 0.2081559579917604, 0.19688529690292403, 0.1887015487710779]\n",
      "val loss history = []\n",
      "Epoch 11/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388, 0.2198793582223948, 0.2081559579917604, 0.19688529690292403, 0.1887015487710779, 0.181701961899038]\n",
      "val loss history = []\n",
      "Epoch 12/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388, 0.2198793582223948, 0.2081559579917604, 0.19688529690292403, 0.1887015487710779, 0.181701961899038, 0.1730901715519511]\n",
      "val loss history = []\n",
      "Epoch 13/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388, 0.2198793582223948, 0.2081559579917604, 0.19688529690292403, 0.1887015487710779, 0.181701961899038, 0.1730901715519511, 0.16731481522822333]\n",
      "val loss history = []\n",
      "Epoch 14/19\n",
      "phase = train\n",
      "train loss history = [0.2482881052378642, 0.23243713159634388, 0.2198793582223948, 0.2081559579917604, 0.19688529690292403, 0.1887015487710779, 0.181701961899038, 0.1730901715519511, 0.16731481522822333, 0.16039898752496234]\n",
      "val loss history = []\n",
      "Epoch 15/19\n",
      "phase = train\n"
     ]
    }
   ],
   "source": [
    "fold = 0\n",
    "for test_case_positive in positive_cases:\n",
    "    \n",
    "    ## Define test datasets\n",
    "    test_case = test_case_positive, negative_cases[fold]\n",
    "    print(test_case)\n",
    "\n",
    "    ### Move patches that correspond to testing case to a new folder (these patches will not be used for training)\n",
    "    for ii in range(len(test_case)):\n",
    "        move_test_case_patches(data_dir, test_case[ii])\n",
    "\n",
    "    # Initialize the model for this run\n",
    "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "    ## Create optimizer\n",
    "    model_ft = model_ft.to(device) ## Send model to GPU\n",
    "    params_to_update = model_ft.parameters()\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                pass\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "    ## Get training ID's for this particular fold\n",
    "    train_idx = []\n",
    "    train_class = []\n",
    "    train_case_0 = []\n",
    "    train_case_1 = []\n",
    "\n",
    "    ## All patches within this loop are train patches\n",
    "    train_dataset = datasets.ImageFolder(root = 'patches//all//')\n",
    "\n",
    "    for i in range(len(train_dataset.imgs)): #Go through all dataset patches\n",
    "        patch_name = train_dataset.imgs[i][0].split('\\\\') ## Grab file name of the patch\n",
    "        patch_case = patch_name[1].split('_')[0] ## Grab case name from file name - relies on \"case_\" format\n",
    "        patch_class = train_dataset.imgs[i][1] ## Grab class of the patch (0 or 1)\n",
    "\n",
    "        ## If patch is within training dataset (i.e. not from testing case)\n",
    "        if patch_case not in test_case:\n",
    "            train_idx.append(i)\n",
    "            train_class.append(patch_class)\n",
    "            if patch_class == 0:\n",
    "                train_case_0.append(patch_case) #Append the case to list of negative training patches\n",
    "            if patch_class == 1:\n",
    "                train_case_1.append(patch_case) #Append the case to list of negative training patches\n",
    "\n",
    "        ## If specific patch belongs to case we are testing on, then a patch has not been correctly moved to test folder\n",
    "        if patch_case in test_case:\n",
    "            print('ERROR - check that test patches are not in training folder')\n",
    "\n",
    "    ## Count how many patches correspond to each case for 0 and 1. Depends on case names being unique in the list (check this manually)\n",
    "    counts_0 = Counter(train_case_0) \n",
    "    counts_1 = Counter(train_case_1)\n",
    "\n",
    "    N_0 = len(train_case_0) #Number of zero patches in training cases\n",
    "    N_1 = len(train_case_1 )#Number of one patches in training cases \n",
    "    print('N_0 = ' + str(N_0))\n",
    "    print('N_1 = ' + str(N_1))\n",
    "\n",
    "    weights_train = []\n",
    "    for i in range(len(train_dataset.imgs)):\n",
    "        if i in train_idx:\n",
    "            patch_name = train_dataset.imgs[i][0].split('\\\\') ## Grab file name of the patch\n",
    "            patch_case = patch_name[1].split('_')[0] ## Grab case name from file name - relies on \"case_\" format\n",
    "            patch_class = train_dataset.imgs[i][1] ## Grab class of the patch (0 or 1)\n",
    "\n",
    "            if patch_class == 0:\n",
    "                weight = (1/counts_0[patch_case])/(N_0+N_1)\n",
    "                weights_train.append(weight)\n",
    "            if patch_class == 1:\n",
    "                weight = 2*(1/counts_1[patch_case])/(N_0+N_1)\n",
    "                weights_train.append(weight)\n",
    "\n",
    "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(weights_train, len(train_dataset), replacement = True)\n",
    "\n",
    "    ## Define dataloaders for downsampled training session\n",
    "    train_image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['all']}\n",
    "    dataloaders_train = {x: torch.utils.data.DataLoader(train_image_datasets[x], batch_size = batch_size, sampler = train_sampler, num_workers=4) for x in ['all']}\n",
    "\n",
    "    print('test case = ' + str(test_case))\n",
    "    print('dataloaders_train = ' + str(dataloaders_train))\n",
    "    # Train and evaluate\n",
    "    model_ft, train_loss_history, all_outputs, all_labels  = train_model(model_ft, fold, dataloaders_train, criterion, optimizer_ft, num_epochs)\n",
    "    print('GPU memory used = ' + str((torch.cuda.memory_reserved() + torch.cuda.memory_allocated())/(1e9)))\n",
    "\n",
    "    # Save model for this fold\n",
    "    PATH = 'resnet_Kfold_' + str(fold) + '_' + str(test_case) + '.pt'\n",
    "    if os.path.isfile(PATH) == True:\n",
    "        print('Model has already been saved to this name. Not over-writing')\n",
    "    else:\n",
    "        torch.save(model_ft, PATH)  \n",
    "\n",
    "    ## Delete variables to clear memory and ensure that learned parameters do not leak between folds\n",
    "    del model_ft, train_loss_history, all_outputs, all_labels, optimizer_ft, params_to_update, \n",
    "\n",
    "    ## Reset all test patches\n",
    "    for ii in range(len(test_case)):\n",
    "        reset_patches(data_dir, test_case[ii])\n",
    "        \n",
    "    print('image patch locations have been reset')\n",
    "    print('')\n",
    "    torch.cuda.empty_cache()\n",
    "    fold += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
